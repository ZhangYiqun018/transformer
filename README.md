# Transformer用于文本情感分析
## 介绍
随着计算机技术的飞速发展，数据的规模与形式呈“爆炸式”的增长，数据挖掘技术在这样一个数值化、信息化的时代则显得尤为重要。随之而来的是对各种模态中提取出精确特征技术的更高要求，如在2016年的美国大选中，传统媒体都推测希拉里将获得大选胜利，而人工智能结合Twitter、Facebook等大数据却分析得出特朗普有可能赢得最后选举，事实证明，人工智能的结论是正确的。在现实生活中， 观点的呈现多以文本的形式，文本数据挖掘技术至2000年开始迅猛发展，现已成为自然语言处理和数据挖掘交叉领域的热点方向，而中情感分析（Sentiment analysis）为text mining的基本问题。

本文旨在使用Transformer的encoder结构实现文本情感分析任务。Transformer是Google于2017年在Attention is all you need一文中提出的基于encoder-decoder结构的深度学习框架。如今，Transform不仅被广泛应用在nlp的各类任务中，甚至在cv领域的一些问题上也展现了出色的性能。

## 数据
### 数据集
数据集源自微博中对话数据集，数据量约8万句，每句话有一个情绪标签，情绪标签有六种，
分别是(Happiness, Love, Sorrow, Fear, Disgust, None)，被标识为(1, 2, 3, 4, 5, 6)

数据集部分展示如下：
| id | sentence | label |
| :--: | :--- | :---:|
| 1 | 我就奇怪了，为啥你能拍的这么美呢？| 2 |
| 2 | 是这是人家自己的事就算我能见到她也不会说你们分手吧什么的可是我真心不喜欢冯绍峰这个理由够吗| 5 |
| 3 |偶看报纸上的评论，好像也不看好。 | 3 |
| ... |... | ... |

## 数据预处理
该数据集比较简单，我对其的预处理就是简单的去除id和无意义的符号，将每句话与标签一一对应。
## 数据集划分
为了防止过拟合，我将原始数据集按照7:3的比例划分为训练集和交叉验证集。
## 评价指标
本文采用的评价指标是基于混淆矩阵计算的F1-score。

## 模型
本次任务是一个分类问题，因此我们只需要Transformer中的encoder部分。
### transformer的实现
#### 多头注意力机制

#### 前馈层

#### 残差网络

#### 位置编码 & 嵌入层

## 实验
### 参数调整
在此问题中，我们一般调整的参数如下表：
| 超参数|
|--|
| |
### 实验结果

